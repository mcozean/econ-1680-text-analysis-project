{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39d50eb6-73bb-4e0c-a594-26a3462ac36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import os\n",
    "import csv \n",
    "import re\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import nltk\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b95fcebc-1b4e-4975-b474-667ee4bf289d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file path for storing raw data\n",
    "file_path = \"/Users/maxwellcozean/Desktop/ECON 1680/Text Analysis Project/Data\"\n",
    "\n",
    "# Join paths and import csv as dataframe\n",
    "df = pd.read_csv(os.path.join(file_path,\"datasciencejobs.csv\"),encoding='utf-8') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c160fad1-6a1d-4797-8019-a30a0bb0fcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop index columns from csv file\n",
    "df = df.drop(['Unnamed: 0','index'],axis=1)\n",
    "\n",
    "# Rename type of ownership column \n",
    "df = df.rename(columns={'Type of ownership': 'Type of Ownership'})\n",
    "\n",
    "# Consolidate null values into a single measure\n",
    "\n",
    "# Replace -1 (int) as null\n",
    "df.replace(-1, np.nan, inplace=True)\n",
    "\n",
    "# Replace -1 (str) as null\n",
    "df.replace('-1', np.nan, inplace=True) \n",
    "\n",
    "# Replace Unknown (str) as null\n",
    "df.replace('Unknown', np.nan, inplace=True) \n",
    "\n",
    "# Replace Unknown / Non-Applicable (str) as null\n",
    "df.replace('Unknown / Non-Applicable', np.nan, inplace=True)\n",
    "\n",
    "# Keep company name (text prior to newline)\n",
    "df['Company Name'] = df['Company Name'].str.split('\\n', expand=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a2d9932-01c8-4bf5-b151-cef17fef52fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split expected annual salary variable into minimum and maximum salary estimates\n",
    "df[['Minimum Salary Estimate','Maximum Salary Estimate']] = df['Salary Estimate'].str.extract(r'\\$(\\d+)K\\s*-\\s*\\$(\\d+)K')\n",
    "\n",
    "# Multiply mimimum salary estimate by 1000\n",
    "df['Minimum Salary Estimate'] = pd.to_numeric(df['Minimum Salary Estimate'])*1000 \n",
    "\n",
    "# Multiply maximum salary estimate by 1000\n",
    "df['Maximum Salary Estimate'] = pd.to_numeric(df['Maximum Salary Estimate'])*1000\n",
    "\n",
    "# Drop null values of minimum and maximum salary estimates (regression output)\n",
    "df = df.dropna(subset=['Minimum Salary Estimate','Maximum Salary Estimate'])\n",
    "\n",
    "# Create single measure of expected salary by taking the average of min/max salary estimates\n",
    "df['Average Salary Estimate'] = (df['Minimum Salary Estimate']+df['Maximum Salary Estimate'])/2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d54893a-fe65-4c2d-9bd2-270bffb183dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define skills dictionary\n",
    "skills_dictionary = [\n",
    "    'Analysis', 'Analysis and design', 'Analysis and reporting', 'Analysis skills',\n",
    "    'Analysis support', 'Analysis techniques', 'Analysis to support', 'Analysis tools',\n",
    "    'Analytic support', 'Analytical abilities', 'Analytical methods', 'Analytical projects',\n",
    "    'Analytical results', 'Analytical support', 'Analytical techniques', 'Analytical tools',\n",
    "    'Analyzing information', 'Quantitative analysis', 'Quantitative and qualitative',\n",
    "    'anova', 'Advanced statistics', 'Data modeling',\n",
    "    'Linear and logistic', 'Linear models', 'Predictive models', 'r-square',\n",
    "    'Regression', 'Statistical analyses', 'Statistical analysis', 'Statistical data analysis',\n",
    "    'Statistical methods', 'Statistical modeling', 'Statistical models', 'Statistical process',\n",
    "    'Statistical reports', 'Statistical techniques', 'Statistical tests', 'Statistics',\n",
    "    'Summarizing data', 'Algorithm', 'Algorithm design', 'Algorithm development', 'Algorithms',\n",
    "    'Algorithms and applications', 'Algorithms and formulations', 'Algorithms in solving real',\n",
    "    'Algorithms to match online', 'AMPL', 'Combinatorial optimization', 'Constraint based',\n",
    "    'Constraint programming', 'Cplex', 'Decision analysis', 'Decision making', 'Decision problems',\n",
    "    'Decision science', 'Decision sciences', 'Decision support', 'Decision support analysis',\n",
    "    'Decision support applications', 'Decision support functions', 'Decision support models',\n",
    "    'Decision support research', 'Decision support research analyst', 'Decision support software',\n",
    "    'Decision support systems', 'Decision support tools', 'Decision tools', 'Decision trees',\n",
    "    'Forecasting', 'Integer', 'Linear programming', 'Mathematical modeling', 'Mathematical models',\n",
    "    'Mathematical programming', 'Nonlinear', 'Non-linear', 'Optimization', 'Quadratic',\n",
    "    'Stochastic optimization', 'MATLAB', 'Model development', 'Model formulation', 'Modeling',\n",
    "    'Network modeling', 'Simulate', 'Simulation', 'Simulations',\n",
    "    'Data analysis', 'Data set', 'Data collection', 'Data gathering', 'Data integrity',\n",
    "    'Data mining', 'Database management', 'Datamart', 'Dataset', 'ERD', 'etl', 'Large data',\n",
    "    'Relational databases', 'Software applications', 'Macros', 'Microsoft Office', 'Microsoft suite', \n",
    "    'Microsoft Word', 'MS Office', 'MS Word', 'o365', 'Power Point', 'PowerPoint', 'Spreadsheet', \n",
    "    'Spreadsheets', 'Word processing','Cognos', 'fixml', 'd3', 'pentaho', 'powerbi', 'tableau',\n",
    "    'Access', 'And access', 'cassandra', 'DB2', 'dbms', 'Hbase', 'Microsoft Access',\n",
    "    'mongodb', 'MS Access', 'Mysql', 'Nosql', 'Oracle', 'SQL Server', 'SQL',\n",
    "    'Teradata', 'tsql', 'XML', 'xsd', 'xsl', 'r', 'SAS', 'SPSS', 'STATA',\n",
    "    'C', 'html', 'Linux', 'Pearl', 'Perl', 'pig', 'Python', 'ruby', 'VBA', 'Visual Basic',\n",
    "    'Azure', 'Google Analytics', 'Hadoop', 'Hive', 'Salesforce', 'SAP', 'Watson'\n",
    "    'Accredited college', 'BA', 'Bachelor of business administration', 'Bachelor of science',\n",
    "    'Bachelors', 'Bachelors degree', 'BS', 'College or university',\n",
    "    'Advanced degree', 'Doctorate', 'Graduate degree', 'Masters', 'Masters ', 'MS degree',\n",
    "    'Master of business administration', 'MBA', 'MBA degree', 'Ph', 'PhD', 'PhD degree']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcbde68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop null values of job description\n",
    "df = df.dropna(subset=['Job Description'])\n",
    "\n",
    "# Define a translation table for the punctuation to be removed\n",
    "table_punctuation = str.maketrans('', '', '!\"#$%&\\'’*+()“”–,-./:;<=>?@[\\\\]^_`{|}~•')\n",
    "\n",
    "# Define set of english stopwords to be removed \n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Define list of stopwords to include\n",
    "include = ['and', 'to', 'or']\n",
    "\n",
    "# Define list of stopwords to remove (all but include)\n",
    "stops = [word for word in nltk_stopwords if word not in include]\n",
    "\n",
    "# Define list of link keywords \n",
    "links = ['https', 'www']\n",
    "\n",
    "# Initialize empty list of cleaned job descriptions\n",
    "clean_description = []\n",
    "\n",
    "# Initialize empty list of extracted skills\n",
    "skills_list = []\n",
    "\n",
    "# For each job description in the dataframe \n",
    "for i, row in enumerate(df['Job Description']):\n",
    "    \n",
    "    # Remove punctuation using the translation table defined above\n",
    "    description = row.translate(table_punctuation)\n",
    "    \n",
    "    # Remove carriage returns from the current description\n",
    "    description = description.replace('\\r',' ')\n",
    "    \n",
    "    # Remove newlines from the current description\n",
    "    description = description.replace('\\n',' ')\n",
    "    \n",
    "    # Remove double spaces (replace with single space)\n",
    "    description = description.replace('  ',' ')\n",
    "    \n",
    "    # Remove stopwords \n",
    "    tokens = [word.lower() for word in nltk.tokenize.word_tokenize(description) if word.lower() not in stops]\n",
    "    \n",
    "    # Remove tokens with links\n",
    "    tokens = [word for word in tokens if not any(link in word for link in links)]\n",
    "    \n",
    "    # Append tokens (with a space) to the cleaned description for each posting\n",
    "    clean_description.append(' '.join(tokens))\n",
    "    \n",
    "    # Intitialize an empty dictionary for the counts of skills within each job description\n",
    "    skill_counts = {}\n",
    "    \n",
    "    # For each skill in the skills dictionary\n",
    "    for skill in skills_dictionary:\n",
    "        \n",
    "        # Update dictionary with the skills found in both the current description \n",
    "        # and the skills dictionary and the number of times they appear\n",
    "        skill_counts[skill] = len(re.findall(r'\\b{}\\b'.format(re.escape(skill)), description, flags=re.IGNORECASE))\n",
    "    \n",
    "    # Initialize empty list of skills for current posting\n",
    "    job_skills = []\n",
    "    \n",
    "    # For each skill and the number of times that skill appears in the job description\n",
    "    for skill, count in skill_counts.items():\n",
    "        \n",
    "        # Add skills that appear multiple times by extending the job skills list\n",
    "        job_skills.extend([skill]*count)\n",
    "    \n",
    "    # Append skills from current description to list of skills from all descriptions\n",
    "    skills_list.append(job_skills)\n",
    "    \n",
    "    # Track progress\n",
    "    # print(round((i / len(df) * 100),3),'%')\n",
    "    \n",
    "# Create column for the cleaned job description\n",
    "df['Cleaned Job Description'] = clean_description\n",
    "\n",
    "# Create column of extracted skills for each posting\n",
    "df['Skills'] = skills_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bd04ad3-f414-414e-bcd3-993405723fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns irrelevant to analysis \n",
    "df = df.drop(['Company Name','Rating','Salary Estimate','Location',\n",
    "              'Headquarters','Size','Founded','Type of Ownership','Sector',\n",
    "              'Revenue','Competitors','Easy Apply'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4774edd1-63aa-4cba-8d99-1583db368e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data\n",
    "df.to_csv(os.path.join(file_path,'datasciencejobs_cleaned.csv'),index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
